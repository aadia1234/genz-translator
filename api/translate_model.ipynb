{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aadi Anand\\OneDrive\\Desktop\\genz-translator\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Slang</th>\n",
       "      <th>Description</th>\n",
       "      <th>Example</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w</td>\n",
       "      <td>shorthand for win</td>\n",
       "      <td>got the job today, big w!</td>\n",
       "      <td>typically used in conversations to celebrate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l</td>\n",
       "      <td>shorthand for loss/losing</td>\n",
       "      <td>i forgot my wallet at home, that’s an l.</td>\n",
       "      <td>often used when referring to a failure or mish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l+ratio</td>\n",
       "      <td>response to a comment or action on the interne...</td>\n",
       "      <td>your tweet got 5 likes and 100 replies calling...</td>\n",
       "      <td>popularized on social media platforms to signi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dank</td>\n",
       "      <td>excellent or of very high quality</td>\n",
       "      <td>that meme is so dank!</td>\n",
       "      <td>commonly used in internet slang to refer to me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cheugy</td>\n",
       "      <td>derogatory term for millennials. used when mil...</td>\n",
       "      <td>that phrase is so cheugy, no one says that any...</td>\n",
       "      <td>used to refer to things that were once popular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>zh</td>\n",
       "      <td>sleeping hour</td>\n",
       "      <td>it’s zh, goodnight!</td>\n",
       "      <td>refers to the time when someone usually goes t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>zomg</td>\n",
       "      <td>oh my god</td>\n",
       "      <td>zomg, i can’t believe you did that!</td>\n",
       "      <td>an exaggerated or enthusiastic version of \"omg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>zot</td>\n",
       "      <td>zero tolerance</td>\n",
       "      <td>our school has a zot policy for bullying.</td>\n",
       "      <td>refers to a strict policy where certain behavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>zup</td>\n",
       "      <td>what’s up?</td>\n",
       "      <td>hey, zup with you today?</td>\n",
       "      <td>a casual way to ask how someone is doing or wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>zzzz</td>\n",
       "      <td>sleeping</td>\n",
       "      <td>i was so tired, i went to bed and was out in s...</td>\n",
       "      <td>represents sleep or someone being asleep, ofte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1779 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Slang                                        Description  \\\n",
       "0           w                                  shorthand for win   \n",
       "1           l                          shorthand for loss/losing   \n",
       "2     l+ratio  response to a comment or action on the interne...   \n",
       "3        dank                  excellent or of very high quality   \n",
       "4      cheugy  derogatory term for millennials. used when mil...   \n",
       "...       ...                                                ...   \n",
       "1774       zh                                      sleeping hour   \n",
       "1775     zomg                                          oh my god   \n",
       "1776      zot                                     zero tolerance   \n",
       "1777      zup                                         what’s up?   \n",
       "1778     zzzz                                           sleeping   \n",
       "\n",
       "                                                Example  \\\n",
       "0                             got the job today, big w!   \n",
       "1              i forgot my wallet at home, that’s an l.   \n",
       "2     your tweet got 5 likes and 100 replies calling...   \n",
       "3                                 that meme is so dank!   \n",
       "4     that phrase is so cheugy, no one says that any...   \n",
       "...                                                 ...   \n",
       "1774                                it’s zh, goodnight!   \n",
       "1775                zomg, i can’t believe you did that!   \n",
       "1776          our school has a zot policy for bullying.   \n",
       "1777                           hey, zup with you today?   \n",
       "1778  i was so tired, i went to bed and was out in s...   \n",
       "\n",
       "                                                Context  \n",
       "0     typically used in conversations to celebrate s...  \n",
       "1     often used when referring to a failure or mish...  \n",
       "2     popularized on social media platforms to signi...  \n",
       "3     commonly used in internet slang to refer to me...  \n",
       "4     used to refer to things that were once popular...  \n",
       "...                                                 ...  \n",
       "1774  refers to the time when someone usually goes t...  \n",
       "1775  an exaggerated or enthusiastic version of \"omg...  \n",
       "1776  refers to a strict policy where certain behavi...  \n",
       "1777  a casual way to ask how someone is doing or wh...  \n",
       "1778  represents sleep or someone being asleep, ofte...  \n",
       "\n",
       "[1779 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"genz_slang.csv\")\n",
    "\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].str.lower()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# This will fallback on the CPU if no CUDA-enabled GPU is available.\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "# model = MyModel()\n",
    "# model.to(device)\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     batch = batch.to(device)\n",
    "#     prediction = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1423/1423 [00:00<00:00, 5558.60 examples/s]\n",
      "Map: 100%|██████████| 356/356 [00:00<00:00, 6033.92 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 01:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.5587364511096968, metrics={'train_runtime': 108.9629, 'train_samples_per_second': 39.178, 'train_steps_per_second': 4.901, 'total_flos': 577774150483968.0, 'train_loss': 0.5587364511096968, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare your dataset\n",
    "train_texts = df['Slang'].tolist()\n",
    "train_labels = df['Description'].tolist()\n",
    "\n",
    "import os\n",
    "# Initialize tokenizer and model\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "model_name = \"t5-small\"  # 250M parameters - best balance of quality and memory usage\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Half precision\n",
    "    device_map=\"auto\"           # Optimize device usage\n",
    ")\n",
    "\n",
    "# Memory optimization for training\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['Slang'], padding=\"max_length\", truncation=True)\n",
    "    targets = tokenizer(examples['Description'], padding=\"max_length\", truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Convert your data into a dataset (using Hugging Face datasets or PyTorch Dataset)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# Tokenize both training and evaluation datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Alternative configuration that avoids fp16 gradient issues\n",
    "# CAUSING PROBLEMS\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Switch to bf16 instead of fp16 if your GPU supports it\n",
    "    bf16=True,                  # Use bfloat16 instead of fp16\n",
    "    fp16=False,                 # Disable fp16\n",
    "    \n",
    "    # Use default optimizer\n",
    "    optim=\"adamw_torch\",           # HuggingFace's implementation\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,  # Added the evaluation dataset here\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example translations:\n",
      "----------------------------------------\n",
      "Input:  This party is lit\n",
      "Output: ,\n",
      "----------------------------------------\n",
      "Input:  No cap\n",
      "Output: ,\n",
      "----------------------------------------\n",
      "Input:  That's fire\n",
      "Output: ,\n",
      "----------------------------------------\n",
      "Input:  I'm so dead\n",
      "Output: ,\n",
      "----------------------------------------\n",
      "Input:  She ate that\n",
      "Output: ,\n",
      "----------------------------------------\n",
      "Input:  I'm finna go to sleep\n",
      "Output: ,\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer to reuse later\n",
    "model_path = \"./results/final_model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Function to translate a sentence using the trained model\n",
    "def translate_sentence(sentence):\n",
    "    # Prepare the input\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = model.device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate translation\n",
    "    output = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=50,  # Adjust as needed for your translations\n",
    "        num_beams=4,    # Beam search for better results\n",
    "        temperature=1.0,  # For more creative outputs, increase this value\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    translated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "# Test with some example sentences\n",
    "test_sentences = [\n",
    "    \"This party is lit\",\n",
    "    \"No cap\",\n",
    "    \"That's fire\",\n",
    "    \"I'm so dead\",\n",
    "    \"She ate that\",\n",
    "    \"I'm finna go to sleep\"\n",
    "]\n",
    "\n",
    "# Translate and display results\n",
    "print(\"Example translations:\")\n",
    "print(\"-\" * 40)\n",
    "for sentence in test_sentences:\n",
    "    translation = translate_sentence(sentence)\n",
    "    print(f\"Input:  {sentence}\")\n",
    "    print(f\"Output: {translation}\")\n",
    "    print(\"-\" * 40)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
